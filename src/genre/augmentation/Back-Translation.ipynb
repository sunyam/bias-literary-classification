{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back-Translation\n",
    "- Back-translate each of the training files in 4 languages and save them as txts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EDA Parameters: N = 25 | alpha = 0.05\n"
     ]
    }
   ],
   "source": [
    "import sys; sys.path.append('../../genre-bias/traditional-machine-learning/')\n",
    "import data_loader\n",
    "import pickle\n",
    "import os\n",
    "from googletrans import Translator\n",
    "\n",
    "with open('../../../train_fnames_scenario_dict_exp_1.pickle', 'rb') as f:\n",
    "    TRAIN_FNAMES = pickle.load(f)\n",
    "    \n",
    "translator = Translator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_translate(text, lang):\n",
    "    \"\"\"\n",
    "    Translates the given English text to 'lang'. Translates that back to English.\n",
    "    Returns the translated English text.\n",
    "    \"\"\"\n",
    "    first = translator.translate(text, src='en', dest=lang)\n",
    "    back = translator.translate(first.text, src=lang, dest='en')\n",
    "    return back.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = ['fr', 'ko', 'de', 'es'] # French, Korean, German, Spanish\n",
    "\n",
    "for scenario in ['A', 'B', 'C', 'D']:\n",
    "    fiction_fnames = TRAIN_FNAMES[scenario]['fiction_fnames']\n",
    "    non_fiction_fnames = TRAIN_FNAMES[scenario]['non_fiction_fnames']\n",
    "\n",
    "    print(\"\\n\\n\\nTotal files for Case {} = {}\".format(scenario, len(fiction_fnames+non_fiction_fnames)))\n",
    "    \n",
    "    for fname in fiction_fnames:\n",
    "        for lang in languages:\n",
    "            text = data_loader.get_passage(fname)\n",
    "            name = fname.split('/')[-1][:-4] + \"__fic__lang_\" + lang + \".txt\"\n",
    "            print(name)\n",
    "            name_path = '../../../data/back-translated/'+name\n",
    "            if os.path.isfile(name_path):\n",
    "                print(\"---------Already exists:\", name_path)\n",
    "                continue\n",
    "                \n",
    "            translated = back_translate(text, lang)\n",
    "            with open(name_path, 'w') as f:\n",
    "                f.write(translated)\n",
    "                \n",
    "    print(\"Done with fiction!\")\n",
    "        \n",
    "    for fname in non_fiction_fnames: # need two \"passages\" per txt\n",
    "        for lang in languages:\n",
    "            two_texts = data_loader.get_passage(fname, two_passages=True)\n",
    "            for i in [0,1]: # back-translate and write both passages\n",
    "                name = fname.split('/')[-1][:-4] + \"__\" + str(i+1) + \"__non__lang_\" + lang + \".txt\"\n",
    "                print(name)\n",
    "                name_path = '../../../data/back-translated/'+name\n",
    "                if os.path.isfile(name_path):\n",
    "                    print(\"----Already exists:\", name_path)\n",
    "                    continue\n",
    "                    \n",
    "                translated = back_translate(two_texts[i], lang)\n",
    "                with open(name_path, 'w') as f:\n",
    "                    f.write(translated)\n",
    "                    \n",
    "    print(\"Done with non-fiction!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1201\n",
      "584\n"
     ]
    }
   ],
   "source": [
    "# Sanity check: check that we have back-translations for one and all\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "with open('../../../train_fnames_scenario_dict_exp_1.pickle', 'rb') as f:\n",
    "    TRAIN_FNAMES = pickle.load(f)\n",
    "\n",
    "outs = []\n",
    "for c in TRAIN_FNAMES:\n",
    "    for dic in TRAIN_FNAMES[c]:\n",
    "        outs.extend(TRAIN_FNAMES[c][dic])\n",
    "        \n",
    "cmp1 = []\n",
    "for i in outs:\n",
    "    cmp1.append(i.split('/')[-1][:-4])\n",
    "    \n",
    "print(len(cmp1))\n",
    "print(len(set(cmp1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2736\n",
      "584\n"
     ]
    }
   ],
   "source": [
    "names = []\n",
    "\n",
    "for i in os.listdir('../../../data/back-translated/'):\n",
    "    names.append(i.split('__')[0])\n",
    "    \n",
    "print(len(names))\n",
    "print(len(set(names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(names) == set(cmp1)\n",
    "# Great!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "txtlab-allen",
   "language": "python",
   "name": "txtlab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
