{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "id": "PiIFklsCQuoi",
    "outputId": "dd788ab4-24da-4df0-abcf-efa693ed4168"
   },
   "source": [
    "### BERT for Genre experiments (with and without augmentation)\n",
    "- We used Google Colaboratory for free GPU access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "4E-MqoeQppFW",
    "outputId": "57b4fa3c-cf4b-401e-a066-a25cf306a931"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train_fnames_scenario_dict_exp_1.pickle',\n",
       " 'data',\n",
       " 'stanford-ner-4.0.0',\n",
       " 'train_fnames_scenario_dict_exp_2.pickle',\n",
       " 'train_fnames_scenario_dict_exp_3.pickle',\n",
       " 'train_fnames_scenario_dict_exp_4.pickle',\n",
       " 'train_fnames_scenario_dict_exp_5.pickle',\n",
       " 'train_fnames_scenario_dict_exp_6.pickle',\n",
       " 'words-500',\n",
       " 'words-1000',\n",
       " 'words-2000',\n",
       " 'words-5000',\n",
       " 'words-10000']"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HOME_PATH = '/content/gdrive/My Drive/txtLAB-2020/bert-run/'\n",
    "import os\n",
    "os.listdir(HOME_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NxriYi8Zr5DW"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random; random.seed(41)\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pickle\n",
    "\n",
    "DATA_PATH = HOME_PATH + 'data/'\n",
    "\n",
    "\n",
    "def get_passage(fname, N, two_passages=False):\n",
    "    \"\"\"\n",
    "    Returns a (continuous) passage of N words from the given txt/fname.\n",
    "    If 'two_passages' is True, it returns two passages (list) instead of one.\n",
    "\n",
    "    Note that the beginning and end (20%) of the txt is skipped.\n",
    "    \"\"\"\n",
    "    with open(fname, 'r') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    all_words = word_tokenize(text)\n",
    "    start = int(0.2*len(all_words))\n",
    "    end = int(len(all_words) - 0.2*len(all_words))\n",
    "\n",
    "    # print(\"Total words: {} | Preview: {}\".format(len(all_words), all_words[10:12]))\n",
    "    # print(\"Start:\", start, \"| End:\", end)\n",
    "\n",
    "    if two_passages:\n",
    "        #assert start+N+N < end\n",
    "        if start+N+N > end:\n",
    "            print(\"Not enough words.. using all the ones avaialable. Total words: {} | Start: {} | End: {}\".format(len(all_words), start, end))\n",
    "            words1 = all_words[start:start+N]\n",
    "            words2 = all_words[start+N:]\n",
    "\n",
    "        else:\n",
    "            words1 = all_words[start:start+N]\n",
    "            words2 = all_words[start+N:start+N+N]\n",
    "        # print(\"Words1: {} | Words2: {}\".format(len(words1), len(words2)))\n",
    "        return [' '.join(words1), ' '.join(words2)]\n",
    "    else:\n",
    "        words = all_words[start:start+N]\n",
    "        # print(\"Words:\", len(words))\n",
    "        return ' '.join(words)\n",
    "\n",
    "\n",
    "######## Train Set ########\n",
    "def load_train_data(scenario, return_ids=False):\n",
    "    \"\"\"\n",
    "    Returns X and Y for training, given the scenario. Also returns the IDs if flag is set to True.\n",
    "    Note: loads two 500-word instances per \"Non-Fiction\" volume.\n",
    "    \"\"\"\n",
    "    # Load train fnames:\n",
    "    fiction_fnames = TRAIN_FNAMES[scenario]['fiction_fnames']\n",
    "    non_fiction_fnames = TRAIN_FNAMES[scenario]['non_fiction_fnames']\n",
    "\n",
    "    if len(fiction_fnames) != 200 and scenario != 'A': # because scenario A has 201 (67+67+67) fnames\n",
    "        N_two_fic = 200 - len(fiction_fnames)\n",
    "        print(\"We have {} fiction fnames\".format(len(fiction_fnames)))\n",
    "\n",
    "    else:\n",
    "        print(\"We have exactly {} fiction fnames.\".format(len(fiction_fnames)))\n",
    "        N_two_fic = 0\n",
    "\n",
    "    assert len(non_fiction_fnames) == 100\n",
    "\n",
    "    print(\"Intersection between fic and nonfic fnames:\", set(fiction_fnames).intersection(set(non_fiction_fnames)))\n",
    "    X = [] # list of training texts\n",
    "    Y = [] # corresponding list of training labels\n",
    "    IDs = [] # corresponding list of unique IDs\n",
    "\n",
    "    if N_two_fic != 0:\n",
    "        print(\"Getting 2 passages from {} files\".format(N_two_fic))\n",
    "        for fname in fiction_fnames[:N_two_fic]:\n",
    "            print(\"Get 2 passages from:\", fname)\n",
    "            fname = fname.replace(\"/Users/sunyambagga/Desktop/txtLAB-2/Augmentation-for-Literary-Data/data/\", DATA_PATH)\n",
    "            X.extend(get_passage(fname, N_WORDS, two_passages=True))\n",
    "            Y.append(\"fic\")\n",
    "            Y.append(\"fic\")\n",
    "            IDs.append(fname.split('/')[-1][:-4]+'__1')\n",
    "            IDs.append(fname.split('/')[-1][:-4]+'__2')\n",
    "    \n",
    "    print(\"X: {} | Y: {}\".format(len(X), len(Y)))\n",
    "\n",
    "    for fname in fiction_fnames[N_two_fic:]:\n",
    "        fname = fname.replace(\"/Users/sunyambagga/Desktop/txtLAB-2/Augmentation-for-Literary-Data/data/\", DATA_PATH)\n",
    "        X.append(get_passage(fname, N_WORDS))\n",
    "        Y.append(\"fic\")\n",
    "        IDs.append(fname.split('/')[-1][:-4])\n",
    "\n",
    "    for fname in non_fiction_fnames: # need two \"passages\" per txt\n",
    "        fname = fname.replace(\"/Users/sunyambagga/Desktop/txtLAB-2/Augmentation-for-Literary-Data/data/\", DATA_PATH)\n",
    "        X.extend(get_passage(fname, N_WORDS, two_passages=True))\n",
    "        Y.append(\"non\")\n",
    "        Y.append(\"non\")\n",
    "        IDs.append(fname.split('/')[-1][:-4]+'__1')\n",
    "        IDs.append(fname.split('/')[-1][:-4]+'__2')\n",
    "\n",
    "    if return_ids:\n",
    "        return np.array(X), np.array(Y), np.array(IDs)\n",
    "    else:\n",
    "        return np.array(X), np.array(Y)\n",
    "\n",
    "\n",
    "######## Test Set ########\n",
    "def load_test_fnames():\n",
    "    \"\"\"\n",
    "    Returns a list of filenames to be used as test-data.\n",
    "    Test Data for all cases: 198 docs (99 \"Non\" & 100 fiction: 33 \"Mys\" + 33 \"Rom\" + 33 \"SciFi\")\n",
    "    \"\"\"\n",
    "    test_path = DATA_PATH + '/Test-Set/'\n",
    "\n",
    "    mys = [test_path+'Mystery_TestSet/'+fname for fname in os.listdir(test_path+'Mystery_TestSet/')]\n",
    "    rom = [test_path+'Romance_TestSet/'+fname for fname in os.listdir(test_path+'Romance_TestSet/')]\n",
    "    sci = [test_path+'SciFi_TestSet/'+fname for fname in os.listdir(test_path+'SciFi_TestSet/')]\n",
    "    fiction_fnames = mys + sci + rom\n",
    "    random.shuffle(fiction_fnames)\n",
    "\n",
    "    non_fiction_fnames = [test_path+'NonNovel_TestSet/'+fname for fname in os.listdir(test_path+'NonNovel_TestSet/')]\n",
    "    print(\"Test Fiction fnames:\", len(fiction_fnames), \"| Test Non-Fiction fnames:\", len(non_fiction_fnames))\n",
    "    return fiction_fnames, non_fiction_fnames\n",
    "\n",
    "\n",
    "def load_test_data():\n",
    "    \"\"\"\n",
    "    Returns X and Y for test set. Also returns a corresponding list of IDs.\n",
    "    \"\"\"\n",
    "    fiction_fnames, non_fiction_fnames = load_test_fnames()\n",
    "\n",
    "    X = [] # list of texts\n",
    "    Y = [] # corresponding list of labels\n",
    "    IDs = [] # corresponding list of unique IDs\n",
    "\n",
    "    for fname in fiction_fnames:\n",
    "        IDs.append(fname.split('/')[-1])\n",
    "        X.append(get_passage(fname, N_WORDS))\n",
    "        Y.append(\"fic\")\n",
    "\n",
    "    for fname in non_fiction_fnames:\n",
    "        IDs.append(fname.split('/')[-1])\n",
    "        X.append(get_passage(fname, N_WORDS))\n",
    "        Y.append(\"non\")\n",
    "\n",
    "    return np.array(X), np.array(Y), np.array(IDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UpnEzK6xycpR"
   },
   "outputs": [],
   "source": [
    "# ######## Train Set with Augmentation ########\n",
    "# def load_train_data_with_EDA(scenario, N_aug=16):\n",
    "#     \"\"\"\n",
    "#     Returns X and Y for training, given the scenario. Data is augmented 16 folds using one of the four EDA techniques at random.\n",
    "#     \"\"\"\n",
    "#     print(\"Generate {} new instances per instance using EDA\".format(N_aug))\n",
    "#     X, Y = load_train_data(scenario)\n",
    "\n",
    "#     augmented_X = X.tolist()\n",
    "#     augmented_Y = Y.tolist()\n",
    "\n",
    "#     operations = [synonym_replacement, random_insertion, random_swap, random_deletion]\n",
    "#     for instance, label in zip(X, Y):\n",
    "#         print(\"X so far:\", len(augmented_X), \"| Y so far:\", len(augmented_Y))#, \"| Y:\", augmented_Y)\n",
    "#         for _ in range(N_aug):\n",
    "#             operation = random.choice(operations)\n",
    "#             new_text = operation(instance)\n",
    "#             augmented_X.append(new_text)\n",
    "#             augmented_Y.append(label)\n",
    "\n",
    "#     return np.array(augmented_X), np.array(augmented_Y)\n",
    "\n",
    "\n",
    "# # Easy Data Augmentation (EDA) techniques from https://www.aclweb.org/anthology/D19-1670.pdf\n",
    "# # As recommended in Table 3, we use alpha=0.05 (for RD, p=alpha); n = 25 | generate 16 instances per training instance\n",
    "# # Note that in order to generate a new instance, randomly choose and perform one of the four EDA operations\n",
    "\n",
    "# import nlpaug.augmenter.word as naw\n",
    "# import nlpaug.model.word_dict as nmw\n",
    "# import random; random.seed(41)\n",
    "# import nltk\n",
    "# from nltk.corpus import stopwords\n",
    "\n",
    "# english_stopwords = stopwords.words('english')\n",
    "# ALPHA = 0.05\n",
    "# N = int(ALPHA*500)\n",
    "# print(\"EDA Parameters: N = {} | alpha = {}\".format(N, ALPHA))\n",
    "\n",
    "# # SR:\n",
    "# def synonym_replacement(text, n=N):\n",
    "#     \"\"\"\n",
    "#     Randomly choose n words from the sentence that are not stop words. Replace each of these words with one of its\n",
    "#     synonyms chosen at random.\n",
    "#     \"\"\"\n",
    "#     aug = naw.SynonymAug(aug_src='wordnet', aug_min=n, aug_max=n, stopwords=english_stopwords)\n",
    "#     augmented_text = aug.augment(text)\n",
    "#     return augmented_text\n",
    "\n",
    "# # ------------------- #\n",
    "\n",
    "# # RS:\n",
    "# def random_swap(text):\n",
    "#     \"\"\"\n",
    "#     Performs random swap N times.\n",
    "#     \"\"\"\n",
    "#     for i in range(N):\n",
    "#         text = random_swap_helper(text)\n",
    "#         # print(\"After run {}, text is {}\".format(i+1, text))\n",
    "#     return text\n",
    "\n",
    "# def random_swap_helper(text):\n",
    "#     \"\"\"\n",
    "#     Randomly choose two words in the sentence and swap their positions.\n",
    "#     \"\"\"\n",
    "#     aug = naw.RandomWordAug(action='swap', aug_min=1, aug_max=1)\n",
    "#     augmented_text = aug.augment(text)\n",
    "#     return augmented_text\n",
    "\n",
    "# # ------------------- #\n",
    "\n",
    "# # RD:\n",
    "# def random_deletion(text, p=ALPHA):\n",
    "#     \"\"\"\n",
    "#     Randomly remove each word in the sentence with probability p=0.05\n",
    "#     \"\"\"\n",
    "#     aug = naw.RandomWordAug(action='delete', aug_p=p)\n",
    "#     augmented_text = aug.augment(text)\n",
    "#     return augmented_text\n",
    "\n",
    "# # ------------------- #\n",
    "\n",
    "# # RI:\n",
    "# def random_insertion(text):\n",
    "#     \"\"\"\n",
    "#     Performs random insertion N times.\n",
    "#     \"\"\"\n",
    "#     for i in range(N):\n",
    "#         text = random_insertion_helper(text)\n",
    "#         # print(\"After run {}, text is: {}\".format(i+1, text))\n",
    "#     return text\n",
    "\n",
    "# def random_insertion_helper(text):\n",
    "#     \"\"\"\n",
    "#     Find a random synonym of a random word in the sentence that is not a stop word.\n",
    "#     Insert that synonym into a random position in the sentence.\n",
    "#     \"\"\"\n",
    "#     original_words = nltk.word_tokenize(text)\n",
    "\n",
    "#     # pick a random word and get its synonyms:\n",
    "#     candidate_syns, candidate_word = get_random_words_synonyms(original_words)\n",
    "\n",
    "#     # pick a random synonym:\n",
    "#     final_synonym = random.choice(candidate_syns)\n",
    "\n",
    "#     # insert at a random position:\n",
    "#     rand_index = random.randint(0, len(original_words)-1)\n",
    "#     original_words.insert(rand_index, final_synonym)\n",
    "\n",
    "#     # print(\"Original word:\", candidate_word)\n",
    "#     # print(\"Final synonym:\", final_synonym)\n",
    "\n",
    "#     return ' '.join(original_words)\n",
    "\n",
    "# def get_random_words_synonyms(original_words):\n",
    "#     \"\"\"\n",
    "#     Helper for RI: picks a random word in 'original_words' which is not a stopword. Returns a list of its synonyms (and the word).\n",
    "#     \"\"\"\n",
    "#     model = nmw.WordNet(lang='eng', is_synonym=True)\n",
    "#     filtered_words = [w for w in original_words if w not in english_stopwords] # remove stopwords\n",
    "#     while True:\n",
    "#         candidate_word = random.choice(filtered_words)\n",
    "#         # print(\"Candidate:\", candidate_word)\n",
    "#         candidate_syns = model.predict(candidate_word)\n",
    "#         # print(\"Before:\", candidate_syns)\n",
    "#         if candidate_word in candidate_syns: # remove all occurrences of candidate_word in candidate_syns\n",
    "#             candidate_syns = list(filter(lambda a: a != candidate_word, candidate_syns))\n",
    "#             # print(\"After:\", candidate_syns)\n",
    "#         if candidate_syns: # return, if not empty\n",
    "#             return candidate_syns, candidate_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tsPgvsF-sXMg"
   },
   "outputs": [],
   "source": [
    "# # Our Custom Data Augmentation (CDA) technique involves (1) Back-Translation, (2) Crossover, (3) Substituting & Deleting Proper Names | generate 16 instances per training instance\n",
    "\n",
    "# from nltk.tag import StanfordNERTagger\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# import random; random.seed(41)\n",
    "\n",
    "\n",
    "# # Using the first-names \"all\" version & surnames \"us\" version from https://github.com/smashew/NameDatabases\n",
    "# with open(HOME_PATH+'data/names/first_names_all.txt', 'r', errors='ignore', encoding='utf8') as r:\n",
    "#     FIRST_NAMES = set(r.read().strip().split('\\n'))\n",
    "    \n",
    "# with open(HOME_PATH+'data/names/surnames_us.txt', 'r', errors='ignore', encoding='utf8') as r:\n",
    "#     LAST_NAMES = set(r.read().strip().split('\\n'))\n",
    "    \n",
    "# print(\"We have unique {} first names and {} last names\".format(len(FIRST_NAMES), len(LAST_NAMES)))\n",
    "\n",
    "\n",
    "# TAGGER = StanfordNERTagger(HOME_PATH+'stanford-ner-4.0.0/classifiers/english.all.3class.distsim.crf.ser.gz',\n",
    "#                            HOME_PATH+'stanford-ner-4.0.0/stanford-ner-4.0.0.jar',\n",
    "#                            encoding='utf-8')\n",
    "\n",
    "\n",
    "# def proper_name_present(text):\n",
    "#     \"\"\"\n",
    "#     Returns -1 if proper name is not present in the given text.\n",
    "#     If present, returns the NER tagged text.\n",
    "#     \"\"\"    \n",
    "#     classified_text = TAGGER.tag(word_tokenize(text)) # NER Tagging\n",
    "\n",
    "#     for word, tag in classified_text:\n",
    "#         if tag == 'PERSON':\n",
    "#             return classified_text\n",
    "        \n",
    "#     # if no tag in the loop is 'PERSON'\n",
    "#     return -1\n",
    "\n",
    "\n",
    "# def proper_names(classified_text, action):\n",
    "#     \"\"\"\n",
    "#     Given the NER-classified text, we can perform two actions: 'delete' or 'substitute'.\n",
    "#     - deletes all proper names\n",
    "#     - substitutes all proper names with random names from https://github.com/smashew/NameDatabases\n",
    "#     \"\"\"\n",
    "#     augmented_text = \"\"\n",
    "    \n",
    "#     for i, tup in enumerate(classified_text):\n",
    "#         word, tag = tup\n",
    "#         surname = False; first_name = False\n",
    "#         if tag == 'PERSON': # for substitue (need to figure out first/surname)\n",
    "#             if action == 'delete':\n",
    "#                 continue\n",
    "            \n",
    "#             elif action == 'substitute':\n",
    "#                 if classified_text[i-1][1] == 'PERSON':\n",
    "#                     surname = True\n",
    "#                 else:\n",
    "#                     first_name = True\n",
    "\n",
    "#                 if first_name: # randomly substitute one\n",
    "#                     augmented_text += \" \" + random.sample(FIRST_NAMES, 1)[0]\n",
    "#                 elif surname:\n",
    "#                     augmented_text += \" \" + random.sample(LAST_NAMES, 1)[0]\n",
    "\n",
    "#         else:\n",
    "#             augmented_text += \" \" + word\n",
    "    \n",
    "#     return augmented_text\n",
    "\n",
    "\n",
    "# def back_translation(fname):\n",
    "#     \"\"\"\n",
    "#     Given the fname, this funciton returns 4 back-translated passages (French, German, Korean, Spanish).\n",
    "    \n",
    "#     See Back-Translation notebook for the translation details.\n",
    "#     \"\"\"\n",
    "#     path = HOME_PATH+'data/back-translated/'\n",
    "#     languages = ['fr', 'ko', 'de', 'es'] # French, Korean, German, Spanish\n",
    "    \n",
    "#     texts = []\n",
    "#     for lang in languages:\n",
    "#         with open(path+fname+'__lang_'+lang+'.txt', 'r') as f:\n",
    "#             t = f.read()\n",
    "#         texts.append(t)\n",
    "#     return texts\n",
    "\n",
    "\n",
    "# def crossover(text1, text2):\n",
    "#     \"\"\"\n",
    "#     Returns a new text instance after performing crossover (index: half of text1).\n",
    "#     First half of text1 + second half of text 2\n",
    "#     \"\"\"\n",
    "#     text1 = text1.split(' ')\n",
    "#     text2 = text2.split(' ')\n",
    "    \n",
    "#     i = int(len(text1)/2)\n",
    "            \n",
    "#     text1_part1 = text1[:i]\n",
    "#     text1_part2 = text1[i:]\n",
    "    \n",
    "#     text2_part1 = text2[:i]\n",
    "#     text2_part2 = text2[i:]\n",
    "    \n",
    "#     new_text = text1_part1 + text2_part2\n",
    "    \n",
    "#     return ' '.join(new_text)\n",
    "\n",
    "\n",
    "# def perform_crossover(main_text, second_texts):\n",
    "#     \"\"\"\n",
    "#     Crossovers the given main_text with each of the second_texts.\n",
    "#     Return len(second_texts) augmented instances\n",
    "#     \"\"\"\n",
    "#     assert len(second_texts) <= 16\n",
    "#     X = []\n",
    "    \n",
    "#     for second in second_texts:\n",
    "#         X.append(crossover(main_text, second))\n",
    "        \n",
    "#     return X\n",
    "\n",
    "\n",
    "# def load_train_data_with_CDA(scenario):\n",
    "#     \"\"\"\n",
    "#     Returns X and Y for training, given the scenario. Data is augmented 16 folds using our CDA technique.\n",
    "#     | Back translation (4) | Crossover (10 or 12) | Proper Name substitution & deletion (2 or 0) |\n",
    "#     \"\"\"\n",
    "#     print(\"Generate 16 new instances per instance using CDA\")\n",
    "#     X, Y, IDs = load_train_data(scenario, return_ids=True)\n",
    "\n",
    "#     augmented_X = X.tolist()\n",
    "#     augmented_Y = Y.tolist()\n",
    "    \n",
    "#     for instance, label, ID in zip(X, Y, IDs):\n",
    "#         print(\"X so far:\", len(augmented_X), \"| Y so far:\", len(augmented_Y))#, \"| Y:\", augmented_Y)\n",
    "        \n",
    "#         # BackTranslation: 4 back-translated augmented instances:\n",
    "#         fname = ID + '__' + label\n",
    "#         translated = back_translation(fname)\n",
    "#         augmented_X.extend(translated)\n",
    "        \n",
    "#         # Check for Proper Names\n",
    "#         ner_tagged = proper_name_present(instance)\n",
    "#         if ner_tagged == -1:\n",
    "#             print(\"No proper names present in\", ID)\n",
    "#             N_crossover = 12\n",
    "        \n",
    "#         else: # if Proper Names exist, get 2 augmented instances (substitute all/delete all)\n",
    "#             pn_deleted = proper_names(ner_tagged, action='delete')\n",
    "#             pn_substituted = proper_names(ner_tagged, action='substitute')\n",
    "#             augmented_X.append(pn_deleted)\n",
    "#             augmented_X.append(pn_substituted)\n",
    "#             N_crossover = 10\n",
    "        \n",
    "#         # Get the rest (10 or 12) crossover augmented instances:\n",
    "#         random_X = random.sample(list(X), N_crossover)\n",
    "#         crossed = perform_crossover(instance, random_X)\n",
    "#         augmented_X.extend(crossed)\n",
    "        \n",
    "#         # Add labels:\n",
    "#         augmented_Y.extend([label]*16)\n",
    "        \n",
    "#     return np.array(augmented_X), np.array(augmented_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "ETjSY0e_r5yK",
    "outputId": "1bf9f21c-691a-4bfd-f47e-61c39af13d9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "# Inspired from: https://mccormickml.com/2019/07/22/BERT-fine-tuning/\n",
    "import torch\n",
    "import random, time, datetime\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "def prepare_dataloader(texts, labels, IDs=[], batch_size=8, max_length=512):\n",
    "    \"\"\"\n",
    "    Takes as input: texts, labels, and corresponding IDs (in case of test-data)\n",
    "    This function returns a DataLoader object.\n",
    "\n",
    "    For train_dataloader, labels are passed. For test_dataloader, both labels and IDs are passed.\n",
    "    BERT tokenizer is used to\n",
    "      (1) Tokenize the sentence.\n",
    "      (2) Prepend the `[CLS]` token to the start.\n",
    "      (3) Append the `[SEP]` token to the end.\n",
    "      (4) Map tokens to their IDs.\n",
    "      (5) Pad or truncate the sentence to `max_length`\n",
    "      (6) Create attention masks for [PAD] tokens.\n",
    "    Authors recommend a batch size of 16/32 for fine-tuning.\n",
    "    \"\"\"\n",
    "    input_ids = []; attention_masks = []\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "    for sent in texts:\n",
    "        encoded_dict = tokenizer.encode_plus(sent, # sentence to encode\n",
    "                                             add_special_tokens=True, # add '[CLS]' and '[SEP]'\n",
    "                                             truncation=True,\n",
    "                                             max_length=512,\n",
    "                                             pad_to_max_length=True,\n",
    "                                             return_attention_mask=True, # construct attention masks\n",
    "                                             return_tensors='pt') # return pytorch tensorss\n",
    "\n",
    "\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask']) # simply differentiates padding from non-padding\n",
    "\n",
    "    # Convert to tensors:\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    labels = torch.tensor(labels)\n",
    "\n",
    "    if IDs == []: # for training data\n",
    "        dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "        print(\"Dataset has input_ids, attention_masks, labels | Length:\", len(dataset))\n",
    "        \n",
    "    else: # for test data\n",
    "        IDs = torch.tensor(IDs)\n",
    "        print(\"Dataset has input_ids, attention_masks, labels, and IDs\")\n",
    "        dataset = TensorDataset(input_ids, attention_masks, labels, IDs)\n",
    "        assert len(dataset) == 198\n",
    "\n",
    "    data_loader = DataLoader(dataset,\n",
    "                             sampler=RandomSampler(dataset),\n",
    "                             batch_size=batch_size)\n",
    "\n",
    "    print(\"Input IDs:\", input_ids.shape)\n",
    "    print(\"Dataset size:\", len(dataset))\n",
    "    return data_loader\n",
    "\n",
    "\n",
    "def train(data_loader, epochs=3):\n",
    "    \"\"\"\n",
    "    Given the data_loader, it fine-tunes BERT for the specific task.\n",
    "    The BERT authors recommend between 2 and 4 training epochs.\n",
    "\n",
    "    Returns fine-tuned BERT model.\n",
    "    \"\"\"\n",
    "    model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "    model.cuda()\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "\n",
    "    total_steps = len(data_loader) * epochs # total number of training steps is [number of batches] x [number of epochs]\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "    total_t0 = time.time() # keep track of time\n",
    "\n",
    "    for epoch_i in range(0, epochs):\n",
    "        print('======== Epoch {:} / {:} ========'.format(epoch_i+1, epochs))\n",
    "        t0 = time.time()\n",
    "        total_train_loss = 0 # reset the total loss for this epoch\n",
    "        model.train() # put the model into training mode\n",
    "\n",
    "        for batch in data_loader:\n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "\n",
    "            model.zero_grad() # clears any previously calculated gradients before performing a backward pass\n",
    "\n",
    "            loss, logits = model(b_input_ids,\n",
    "                                 token_type_ids=None,\n",
    "                                 attention_mask=b_input_mask,\n",
    "                                 labels=b_labels)\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # clip the norm of the gradients to 1.0 to help prevent the \"exploding gradients\" problem\n",
    "            optimizer.step() # update parameters and take a step using the computed gradient\n",
    "            scheduler.step() # update the learning rate\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(data_loader)\n",
    "        training_time = format_time(time.time() - t0)\n",
    "\n",
    "        print(\"\\tAverage training loss: {0:.2f}\".format(avg_train_loss))\n",
    "        print(\"\\tTraining epcoh took: {:}\".format(training_time))\n",
    "    print(\"\\n\\nTraining complete\\nTotal training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def predict(model, data_loader):\n",
    "    \"\"\"\n",
    "    Given the fine-tuned model and data loader, it returns flat predictions, list of prob(fiction), and corresponding true-labels & IDs.\n",
    "\n",
    "    For predictions, we pick the label (0 or 1) with the higher score. The output for each batch are a 2-column ndarray (one column for \"0\"\n",
    "    and one column for \"1\"). Pick the label with the highest value and turn this in to a list of 0s and 1s.\n",
    "    \"\"\"\n",
    "    model.eval() # put model in evaluation mode\n",
    "\n",
    "    predictions, prob_fiction, true_labels, IDs = [], [], [], []\n",
    "\n",
    "    for batch in data_loader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        b_input_ids, b_input_mask, b_labels, b_IDs = batch\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(b_input_ids, token_type_ids=None,\n",
    "                          attention_mask=b_input_mask)\n",
    "\n",
    "        logits = outputs[0]\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        labels = b_labels.to('cpu').numpy()\n",
    "        ids = b_IDs.to('cpu').numpy()\n",
    "\n",
    "        predictions.append(logits)\n",
    "        true_labels.append(labels)\n",
    "        IDs.append(ids)\n",
    "\n",
    "\n",
    "    flat_predictions = np.concatenate(predictions, axis=0)\n",
    "\n",
    "    probs = torch.nn.functional.softmax(torch.from_numpy(flat_predictions), dim=-1) # convert logits to probabilities\n",
    "    prob_fiction = probs[:,1] # because order is [0,1] and 1 is fiction\n",
    "    prob_fiction = prob_fiction.numpy()\n",
    "\n",
    "    flat_predictions = np.argmax(flat_predictions, axis=1).flatten() # pick the one with the highest value\n",
    "\n",
    "    flat_true_labels = np.concatenate(true_labels, axis=0)\n",
    "    flat_IDs = np.concatenate(IDs, axis=0)\n",
    "\n",
    "    return flat_predictions, prob_fiction, flat_true_labels, flat_IDs\n",
    "\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mdsuwnv8t2-1"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score #, precision_score, recall_score, accuracy_score, average_precision_score\n",
    "\n",
    "def run_bert():\n",
    "    \"\"\"\n",
    "    Runs the BERT model:\n",
    "    1) Prepares data loaders.\n",
    "    2) Fine-tunes the BERT model.\n",
    "    3) Returns the predictions on the test set.\n",
    "    \"\"\"\n",
    "    # DataLoader:\n",
    "    train_dataloader = prepare_dataloader(texts=X_train, labels=labels_train)\n",
    "    \n",
    "    print(\"Beginning training now..\")\n",
    "    # Train/fine-tune:\n",
    "    bert_model = train(train_dataloader)\n",
    "\n",
    "    # Predict on test set:\n",
    "    test_dataloader = prepare_dataloader(texts=X_test, labels=labels_test, IDs=testIDs_idx)\n",
    "    predictions, prob_fiction, true_labels, IDs_idx = predict(bert_model, test_dataloader)\n",
    "    print(\"Predictions: {}\\n\\nLabels:{}\\n\\nIDs_idx:{}\".format(predictions, true_labels, IDs_idx))\n",
    "    print(\"\\n\\n\\n\\nF1=\", f1_score(true_labels, predictions, pos_label=1))\n",
    "    write_predictions(IDs_idx, prob_fiction, predictions)\n",
    "\n",
    "\n",
    "\n",
    "def write_predictions(IDs_idx, prob_fiction, predictions):\n",
    "    # Save predictions:\n",
    "    print(\"Write predictions to:\", preds_path)\n",
    "\n",
    "    with open(preds_path, 'w') as f:\n",
    "        f.write('fname\\tprobability_fiction\\tlabel\\n')\n",
    "        for index, prob, pred in zip(IDs_idx, prob_fiction, predictions):\n",
    "            ID = test_IDs[int(index)]\n",
    "\n",
    "            if prob >= 0.5:\n",
    "                f.write(ID+'\\t'+str(prob)+'\\tfic\\n')\n",
    "                assert pred == 1\n",
    "            else:\n",
    "                f.write(ID+'\\t'+str(prob)+'\\tnon\\n')\n",
    "                assert pred == 0\n",
    "\n",
    "\n",
    "def labels_str_to_int(Y):\n",
    "    \"\"\"\n",
    "    Given the input labels, it converts them to integeres (fiction: 1 | non-fiction: 0)\n",
    "    \"\"\"\n",
    "    labels = []\n",
    "    for l in Y:\n",
    "        if l == 'fic':\n",
    "            labels.append(1)\n",
    "        elif l == 'non':\n",
    "            labels.append(0)\n",
    "        else:\n",
    "            print(\"Error:\", l)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "PBQUHkoZiNGW",
    "outputId": "d947fd16-824a-4b3a-864b-6ce9c17dc08e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write predictions to: /content/gdrive/My Drive/txtLAB-2020/bert-run/words-10000/exp_6_BERT_preds_for_Case_D.tsv\n",
      "We have 178 fiction fnames\n",
      "Intersection between fic and nonfic fnames: set()\n",
      "Getting 2 passages from 22 files\n",
      "Get 2 passages from: /Users/sunyambagga/Desktop/txtLAB-2/Augmentation-for-Literary-Data/data/NovelEnglish_Romance/2015_Lewis,RJ_Leah_ROM.txt\n",
      "Get 2 passages from: /Users/sunyambagga/Desktop/txtLAB-2/Augmentation-for-Literary-Data/data/NovelEnglish_Romance/2014_Raine,Alice_TheDarknessWithinHim_ROM.txt\n",
      "Get 2 passages from: /Users/sunyambagga/Desktop/txtLAB-2/Augmentation-for-Literary-Data/data/NovelEnglish_Romance/2015_Wood,Jessica_PromisetoMarry_ROM.txt\n",
      "Not enough words.. using all the ones avaialable. Total words: 30496 | Start: 6099 | End: 24396\n",
      "Get 2 passages from: /Users/sunyambagga/Desktop/txtLAB-2/Augmentation-for-Literary-Data/data/NovelEnglish_Romance/2015_Scott,Ginger_WildReckless_ROM.txt\n",
      "Get 2 passages from: /Users/sunyambagga/Desktop/txtLAB-2/Augmentation-for-Literary-Data/data/NovelEnglish_Romance/2015_King,CM_ICherishYourHeart_ROM.txt\n",
      "Get 2 passages from: /Users/sunyambagga/Desktop/txtLAB-2/Augmentation-for-Literary-Data/data/NovelEnglish_Romance/2014_Netzel,AC_TheCasualRule_ROM.txt\n",
      "Get 2 passages from: /Users/sunyambagga/Desktop/txtLAB-2/Augmentation-for-Literary-Data/data/NovelEnglish_Romance/2015_Ryan,Kendall_TheGentlemanMentor_ROM.txt\n",
      "Get 2 passages from: /Users/sunyambagga/Desktop/txtLAB-2/Augmentation-for-Literary-Data/data/NovelEnglish_Romance/2015_Cruise,Marilyn_TheBlackChapel_ROM.txt\n",
      "Get 2 passages from: /Users/sunyambagga/Desktop/txtLAB-2/Augmentation-for-Literary-Data/data/NovelEnglish_Romance/2015_Callihan,Kristen_TheFriendZone_ROM.txt\n",
      "Get 2 passages from: /Users/sunyambagga/Desktop/txtLAB-2/Augmentation-for-Literary-Data/data/NovelEnglish_Romance/2013_Marton,Dana_Deathtrap_ROM.txt\n",
      "Get 2 passages from: /Users/sunyambagga/Desktop/txtLAB-2/Augmentation-for-Literary-Data/data/NovelEnglish_Romance/2013_Meyer,Tijan_CarterReed_ROM.txt\n",
      "Get 2 passages from: /Users/sunyambagga/Desktop/txtLAB-2/Augmentation-for-Literary-Data/data/NovelEnglish_Romance/2015_Ryan,Kaylee_JustSayWhen_ROM.txt\n",
      "Get 2 passages from: /Users/sunyambagga/Desktop/txtLAB-2/Augmentation-for-Literary-Data/data/NovelEnglish_Romance/2015_Foor,Jennifer_Binge_ROM.txt\n",
      "Get 2 passages from: /Users/sunyambagga/Desktop/txtLAB-2/Augmentation-for-Literary-Data/data/NovelEnglish_Romance/2015_Edwards,Nicole_Brendon_ROM.txt\n",
      "Get 2 passages from: /Users/sunyambagga/Desktop/txtLAB-2/Augmentation-for-Literary-Data/data/NovelEnglish_Romance/2014_Hoover,Colleen_UglyLove_ROM.txt\n",
      "Get 2 passages from: /Users/sunyambagga/Desktop/txtLAB-2/Augmentation-for-Literary-Data/data/NovelEnglish_Romance/2015_Meyer,Marie_AcrosstheDistance_ROM.txt\n",
      "Get 2 passages from: /Users/sunyambagga/Desktop/txtLAB-2/Augmentation-for-Literary-Data/data/NovelEnglish_Romance/2014_Elliot,Kendra_Vanished_ROM.txt\n",
      "Get 2 passages from: /Users/sunyambagga/Desktop/txtLAB-2/Augmentation-for-Literary-Data/data/NovelEnglish_Romance/2014_York,Zoe_LoveinaSmallTown_ROM.txt\n",
      "Get 2 passages from: /Users/sunyambagga/Desktop/txtLAB-2/Augmentation-for-Literary-Data/data/NovelEnglish_Romance/2015_Armentrout,JenniferL_FallWithMe_ROM.txt\n",
      "Get 2 passages from: /Users/sunyambagga/Desktop/txtLAB-2/Augmentation-for-Literary-Data/data/NovelEnglish_Romance/2015_Mosteller,MistiD_TheFatRules_ROM.txt\n",
      "Get 2 passages from: /Users/sunyambagga/Desktop/txtLAB-2/Augmentation-for-Literary-Data/data/NovelEnglish_Romance/2015_Mathewson,RL_DoubleDare_ROM.txt\n",
      "Get 2 passages from: /Users/sunyambagga/Desktop/txtLAB-2/Augmentation-for-Literary-Data/data/NovelEnglish_Romance/2015_MacLean,Julianne_TheColorofJoy_ROM.txt\n",
      "X: 44 | Y: 44\n",
      "Test Fiction fnames: 99 | Test Non-Fiction fnames: 99\n",
      "testIDs indexes: 198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1770: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has input_ids, attention_masks, labels | Length: 400\n",
      "Input IDs: torch.Size([400, 512])\n",
      "Dataset size: 400\n",
      "Beginning training now..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 3 ========\n",
      "\tAverage training loss: 0.27\n",
      "\tTraining epcoh took: 0:00:40\n",
      "======== Epoch 2 / 3 ========\n",
      "\tAverage training loss: 0.06\n",
      "\tTraining epcoh took: 0:00:41\n",
      "======== Epoch 3 / 3 ========\n",
      "\tAverage training loss: 0.01\n",
      "\tTraining epcoh took: 0:00:43\n",
      "\n",
      "\n",
      "Training complete\n",
      "Total training took 0:02:05 (h:mm:ss)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:53: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has input_ids, attention_masks, labels, and IDs\n",
      "Input IDs: torch.Size([198, 512])\n",
      "Dataset size: 198\n",
      "Predictions: [1 1 1 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0 1 1 1 0 0 1 1 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 0 1 1 0 0 0 0 1 0 0 1 0 0 1 1 0 0 0 0\n",
      " 1 0 0 0 1 1 0 1 0 1 1 1 0 1 1 0 1 1 0 1 1 0 0 0 0 0 1 0 0 1 0 0 0 0 1 1 1\n",
      " 1 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 1 1 0 0 0 0 1 0\n",
      " 0 0 1 1 1 0 0 1 1 1 0 1 0 0 1 0 1 0 0 1 1 0 0 0 0 0 0 0 1 1 1 0 0 0 1 0 0\n",
      " 1 0 0 0 0 1 1 1 1 1 0 1 1]\n",
      "\n",
      "Labels:[1 1 1 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0 1\n",
      " 0 0 0 1 1 0 0 0 1 0 0 0 1 1 1 1 0 0 0 1 1 0 0 0 1 1 0 1 1 0 0 1 1 0 0 1 0\n",
      " 1 0 0 1 1 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 1 0 0 0 0 0 1 1 0 1 0 0 0 1 1 1 1\n",
      " 1 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0 0 1 1 1 0 0 0 0 1 1\n",
      " 0 0 1 1 1 0 1 1 1 1 0 1 1 1 1 0 1 0 1 1 1 1 0 0 0 0 0 0 1 1 1 1 0 0 1 0 1\n",
      " 1 0 0 1 0 1 1 1 1 1 0 1 1]\n",
      "\n",
      "IDs_idx:[  3.  43.  68. 108. 140. 110. 162.  76. 148. 166. 188. 132. 107.   0.\n",
      " 114. 160.  60. 147.  18. 184. 179. 159. 195. 189.  35.  27.  90.   2.\n",
      "  38.  44.   7.  74. 181. 102. 113. 143.  36. 177. 167. 141.  30.  33.\n",
      " 118. 163. 154.  34. 191. 153. 190.   5.  96.  31.   1. 158. 109. 151.\n",
      "  70.  69. 130. 172. 117.  14.  88. 192.  48.  62. 136. 103.  63.  75.\n",
      " 173. 146.  78. 116.  95. 131. 138.   8.  66.  64.  40.  37. 145.  26.\n",
      "  94.  19. 196.  45.  84. 175.  22.  12. 120.  71.  13. 174. 186. 150.\n",
      " 157. 106.  29.  11. 112.  32. 126. 156. 105.  15.  50.  41.  28.   4.\n",
      " 178. 165.  98. 127. 128.  83. 185. 197. 122.   9.  39. 161. 168. 152.\n",
      " 187. 123.  97. 182. 144.  58. 194.  21.  23. 115. 137. 183. 125.  92.\n",
      "  46.  55. 169. 171. 193. 101.  91.  81. 149. 124.  59.  52.  16.  99.\n",
      "  86.  53.  49.  87. 135.  82.  57.  47.  20. 104.  67. 142.  85.  25.\n",
      "  77.  80. 133. 129. 121. 139. 155. 134.  42.  54.  73.  65. 164. 119.\n",
      "  17. 180.  72.  10. 111. 170.  93. 176.  51.  89.  79.  56.   6. 100.\n",
      "  24.  61.]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "F1= 0.8685714285714284\n",
      "Write predictions to: /content/gdrive/My Drive/txtLAB-2020/bert-run/words-10000/exp_6_BERT_preds_for_Case_D.tsv\n"
     ]
    }
   ],
   "source": [
    "N_WORDS = 10000\n",
    "experiment = 'train_fnames_scenario_dict_exp_6.pickle'\n",
    "SCENARIO = 'D'\n",
    "\n",
    "preds_path = HOME_PATH + 'words-' + str(N_WORDS) + '/' + experiment.split('.')[0].split('dict_')[1]+'_BERT_preds_for_Case_'+SCENARIO+'.tsv'\n",
    "print(\"Write predictions to:\", preds_path)\n",
    "\n",
    "with open(HOME_PATH+experiment, 'rb') as f: # contains training novels for each scenario\n",
    "    TRAIN_FNAMES = pickle.load(f)\n",
    "\n",
    "\n",
    "# Load training data:\n",
    "# if CDA: # with CDA\n",
    "#     X_train, Y_train = load_train_data_with_CDA(SCENARIO)\n",
    "\n",
    "# else: # wihtout any Data Augmentation\n",
    "X_train, Y_train = load_train_data(SCENARIO)\n",
    "X_train = X_train.tolist(); Y_train = Y_train.tolist() # convert to list\n",
    "labels_train = labels_str_to_int(Y_train) # convert labels to integers\n",
    "\n",
    "# Test data:\n",
    "X_test, Y_test, test_IDs = load_test_data()\n",
    "X_test = X_test.tolist(); Y_test = Y_test.tolist(); test_IDs = test_IDs.tolist() # convert to list\n",
    "\n",
    "# Sanity check:\n",
    "if SCENARIO == 'A':\n",
    "    assert len(X_train) == len(Y_train) == 401\n",
    "else:\n",
    "    assert len(X_train) == len(Y_train) == 400\n",
    "\n",
    "assert len(X_test) == len(Y_test) == 198\n",
    "\n",
    "labels_test = labels_str_to_int(Y_test) # convert labels to integers\n",
    "testIDs_idx = np.linspace(0, len(test_IDs), len(test_IDs), False) # can't create a tensor of strings, so create a corresponding list of indexes; we use that to index into test_IDs\n",
    "print(\"testIDs indexes:\", len(testIDs_idx))\n",
    "\n",
    "run_bert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4xLd6uQ93soG"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BERT-Genre-EDA-CDA.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
